---
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
## BDA 503  -- BERK ORBAY - FINAL EXAM

## DEVRIM NESIPOGLU

## -------------------------------------------------------------------

## PART I . 1 

## 1.1.QUESTION. 
What is your opinion about two y-axis graphs? 
Do you use it at work? 
Is it a good practice, a necessary evil, or plain horrible? 

## 1.1.ANSWER.
with dual axis graph, you can summarize or plot two y axis variables that have 
different variables that have different domains.
For example, you can plot the number of cases on one axis and the mean salary on another.
Also this chart can also be a mix of different graphic elements so that the dual y axis 
chart encompasses several of the different chart types types.It may display the counts
as a line and the mean of each category as a bar.
There are very few situations where it is appropriate to use two different scales on 
the same plot.It is very easy to mislead and confuse the viewer of the graphic. 
So I can say, it will change up to information that I want to visualize. 
If it makes graph difficult to understand, and creates confusion, I will not use.


## PART I . 2 
## 1.2. QUESTION
What is your exploratory data analysis workflow? Suppose you are given a data set and a research question. 
Where do you start? How do you proceed? For instance, you are given the task to distribute funds from donations to public welfare projects in a wide range of subjects.
with the objective of maximum positive impact on the society in general. 

## 1.2. ANSWER
- Exploratory data analysis workflow
Aim of the EDA is to understand data better. 
Easiest way is to use questions as tools to guide investigation. 
With the help of questions, you can find chance to dig deep inside data, 
have knowledge about pattern and structure or realize handicaps and challenges. 
After you gain ideas of about data, you can start for data cleaning.
Not Applicable values, characters that creates problem, outliers will be cleaned
after investigation of data.
* Generate questions about data.
* Search for answers by visualizing, transforming, and modelling your data.
* Use what you learn to refine your questions and/or generate new questions.
- How do you measure impact? 
I will look at the data regarding questionnaire at drop in visits to people who take funds.
I will compare with data before the the fund given and after the people used fund.According to aim, I will look the difference and measure performance of project.
To be honest, I would choose the title  “Pain Points in Our Society and Optimal Budget Allocation”?


## PART I . 3 

## 1.3. QUESTION
What are the differences between time series and non time series data in terms of analysis, modeling and validation? In other words what makes Bit coin price movements analysis 
different from diamonds (or carat) data set?

## 1.3. ANSWER
A time series is a series of data points indexed (or listed or graphed) in time order. 
Time series are a series of observations made over a certain time interval. 
It is commonly used in economic forecasting as well as analyzing climate data over large periods of time. The main idea behind time series analysis is to use a certain number of previous observations to predict future observations.Any metric that is measured over regular time intervals makes a Time Series. 
Example: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.

Time series models are very useful models when you have serially correlated data. Most of business houses work on time series data to analyze sales number for the next year, website traffic, competition position and much more. However, it is also one of the areas, which many analysts do not understand.

The biggest difference is that time series regression accounts for the autocorrelation between time events, which always exists, while in normal regression, independence of  serial errors are presumed, or at least minimized.

The Bit coin is a digital currency which has recently emerged as a peer-to-peer payment system
to facilitate transactions. It is not issued by any central bank or other financial institution 
but uses cryptographic methods and relies on an open-source software algorithm which verifies 
decentralized transactions and controls the creation of new Bit coins. 
At Bit coin price movement analysis, we can identify pattern.Date of observation of bit coin 
prices can be used.

At diamonds data, there is no time events for that reason we cannot analyze with time series model

## PART I . 4 

## 1.4. QUESTION
If you had to plot a single graph using the data below what would it be? Why? 
Make your argument,actually code the plot and provide the output. 
(You can find detailed info about the movies data set in its help file. Use ?movies, after you load ggplot2movies package

## 1.4. ANSWER
I want to look for if there is a relationship between longer movies with length less than 240 minutes 
and the number of votes the movies received on IMDB. Also use the alpha parameter to geom_point to deal with over-plotting and use the geom_smooth function to add a regression line (without confidence bands) 
to the plot.
```{r}
library(ggplot2movies)
library(tidyverse)
```

```{r,, fig.width=6,fig.height=3}
movies %>% filter(length < 240) %>%
  ggplot(aes(x = votes, y = length)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

```

## PART II . Extending Your Group Project 

## 2. QUESTION
In this part you are going to extend your group project with an additional 
analysis supported by some visualizations. 
You are tasked with finding the best improvement on the top of your group project.
About one page is enough, two pages tops.

## 2. ANSWER

## "We care about your needs!"

```{r}
#Sys.setlocale(locale="Turkish_Turkey.1254")
#knitr::opts_chunk$set(echo = FALSE)

#setwd("C:/Users/Dell_User/Documents/GitHub/Final")
options(Encoding="UTF-8")
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggplot2)
library(dplyr) 
library(scales)
library(stringr) 
library(reshape)
library(arules)
library(arulesViz)
library(kableExtra)
```

##  How People Order Online? Analysis of 3 million Instacart orders.
##  BDA-503 - Term Project - Berk Orbay  


As group **cleveR**, we will work on the online order behaviors of [Instacart](https://www.instacart.com/) 

## Departments
This file contains the names of the departments with their department_id and name of the department.

* department_id: department identifier
* department: the name of the department

```{r, echo=FALSE}
departments<-read.csv("data/departments.csv")
kable(head(departments,5),align="l")

```

## Orders

This file gives a list of all orders we have in the data set. 1 row per order. For example, we can see that user 1 has 11 orders, 1 of which is in the train set, and 10 of which are prior orders. The orders.csv doesn't tell us about which products were
```{r, echo=FALSE}
orders<-read.csv("data/orders.csv" ,nrows = 100000)
#orders<-read.csv("data/orders.csv")
kable(head(orders,5),align="l")

```

### How many prior orders are there?
We can see that there are always at least 3 prior orders.


```{r, fig.width = 4, fig.height = 3}
orders %>% filter(eval_set=="prior") %>% count(order_number) %>% ggplot(aes(order_number,n)) + geom_line(color="red", size=1)+geom_point(size=2, color="green")


```

## How many items do people buy?

Let’s have a look how many items are in the orders. We can see that people most often order 
around 5 items. The distributions are comparable between the train and prior order set.

Order_Products_Prior (op_prior) gives us information about which products (product_id) were ordered. 
It also contains information of the order (add_to_cart_order) in which the products 
were put into the cart and information of whether this product is a re-order(1) or not(0).

```{r}
orders<-read.csv("data/orders.csv" ,nrows = 100000)
#orders<-read.csv("data/orders.csv")
kable(head(orders,5),align="l")

op_train<-read.csv("data/order_products__train.csv",nrows = 100000)
#op_train<-read.csv("data/order_products__train.csv")
kable(head(op_train,5),align="l")

```

```{r}
op_prior<-read.csv("data/order_products__prior.csv",nrows = 100000)
kable(head(op_prior,4),align="l")
```

  
```{r, fig.width = 4, fig.height = 3}
op_prior %>% 
  group_by(order_id) %>% 
  summarize(n_items = last(add_to_cart_order)) %>%
  ggplot(aes(x=n_items))+
  geom_histogram(stat="count",fill="red") + 
  geom_rug() + 
  coord_cartesian(xlim=c(0,80))
```
  
Order_Products_Train (op_train) gives us information about which products (product_id) were ordered. 
It also contains information of the order (add_to_cart_order) in which the products were put 
into the cart and information of whether this product is a re-order(1) or not(0).

## Train Set

```{r, fig.width = 4, fig.height = 3}
op_train %>% 
  group_by(order_id) %>% 
  summarize(n_items = last(add_to_cart_order)) %>%
  ggplot(aes(x=n_items))+
  geom_histogram(stat="count",fill="red") + 
  geom_rug()+
  coord_cartesian(xlim=c(0,80))

```

```{r, echo=FALSE}
op_train<-read.csv("data/order_products__train.csv",nrows = 100000)
#op_train<-read.csv("data/order_products__train.csv")
kable(head(op_train,5),align="l")

```

## PART III: Welcome to Real Life 

## 3. QUESTION:
Gather data from Higher Education Council’s (YÖK) data service. https://istatistik.yok.gov.tr/ . 
Choose an interesting theme which can be analyzed with the given data and collect relevant data 
from the service. 
a) Gather the data, bind them together and save in an .RData file. Make .RData file available 
online for everybody. Provide the data link in your analysis.
b) Perform EDA on the data you collected based on the theme you decided on. 

## 3. ANSWER:

```{r}
suppressMessages(library(tidyverse))
library(dplyr)
library(knitr)
library(ggplot2)
library(shiny)
student_info <- read.csv("C:/Users/Dell_User/Documents/GitHub/Final/Student Numbers RawData.csv", sep=",")
setwd("C:/Users/Dell_User/Documents/GitHub/pj-nesipoglud/Final")
download.file("https://mef-bda503.github.io/pj-nesipoglud/files/Final_R.RData","Final_R.RData")
dat <- get(load("Final_R.RData")) %>%tbl_df() 
```


```{r}
print("Education System of Turkey data row number")
nrow(student_info)
```

```{r, echo=FALSE }
Sys.setlocale(locale = "Turkish_Turkey.1254")
```

## ANALYSIS OF HIGHER EDUCATION SYSTEM OF TURKEY

```{r}
names(student_info)[1]<-"University_Name"
names(student_info)[2]<-"Year_of_Organization"
names(student_info)[3]<-"Type"
names(student_info)[4]<-"Province"
names(student_info)[5]<-"Region"
names(student_info)[6]<-"Male_Vocational_Training_School_Students"
names(student_info)[7]<-"Female_Vocational_Training_School_Students"
names(student_info)[8]<-"Total_Vocational_Training_School_Students"
names(student_info)[9]<-"Male_Undergraduate_Students"
names(student_info)[10]<-"Female_Undergraduate_Students"
names(student_info)[11]<-"Total_Undergraduate_Students"
names(student_info)[12]<-"Male_Master_Students"
names(student_info)[13]<-"Female_Master_Students"
names(student_info)[14]<-"Total_Master_Students"
names(student_info)[15]<-"Male_Doctorate_Students"
names(student_info)[16]<-"Female_Doctorate_Students"
names(student_info)[17]<-"Total_Doctorate_Students"
names(student_info)[18]<- "Male_Grand_Total"
names(student_info)[19]<-"Female_Grand_Total"
names(student_info)[20]<-"Grand_Total"
```

After tidying data and have exploratory data analysis, we can start for getting deep inside the data

```{r}
names(student_info)
```
```{r}
head(student_info)
```


```{r}
#Before tidying data set
summary(student_info)

```

```{r,fig.width = 10, fig.height = 3}
ggplot(aes(x=Region), data=subset(student_info, !is.na(Region))) +
  geom_bar(aes(fill=Region), color='black') +
  labs(x= 'Region', y='Number of Students', title= 'Region Frequency Diagram')
```
## University Numbers by University Type


```{r}
student_info %>%
  count(Type)
```

## Year of Organizations of Universities

```{r,fig.width = 10, fig.height = 4}
ggplot(aes(x=Year_of_Organization), data=subset(student_info, !is.na(Year_of_Organization))) +
  geom_bar(aes(fill=Year_of_Organization), color='green') +
  labs(x= 'Year of Organization', y='Number of Students', title= 'Year of Organization Frequency Diagram')
```

Let's look year of organizations of universities by Region

```{r,fig.width = 10, fig.height = 4}
ggplot(data = student_info) + 
  geom_point(mapping = aes(x = Year_of_Organization, y = Region)) +
    labs(x= 'Year of Organization', y='Region', title= 'Year of Organization by Region Frequency')

```

## Number of students by university type

```{r}
student_info %>% group_by(Type) %>%  summarise(quota=sum(Grand_Total)) %>% arrange(desc(Type))

```

```{r,fig.width = 7, fig.height = 4}
ggplot(student_info, aes(Type,Total_Master_Students)) +
  geom_point(aes(color = Region)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Total Master students almost half number at foundation universities",
    subtitle = "Mostly Marmara and Central Anatolia regions have more master students than the other regions"
  )

```
With this graph, establishment of new universities analyzed in 20 years of period by university type and and Region. 

```{r}
ggplot(data=subset(student_info, !is.na(Region) & !is.na(Type)), aes(x=Type, y=Year_of_Organization)) +
  geom_boxplot(aes(fill=Type), color='black') +
  facet_wrap(~Region) +
  stat_summary(fun.y = mean, geom="point", size=2) +
  labs(x= 'Region', y= 'Year of Organization', title= "Year of Organization vs. University Type by Gearbox")

```

## Reference
Data is download from https://istatistik.yok.gov.tr/






